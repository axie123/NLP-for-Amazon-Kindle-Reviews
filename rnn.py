# -*- coding: utf-8 -*-
"""main2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1upgiAmDB95dq-g94yjIjlfOolkQT9ntK
"""

import torch 
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import numpy as np
import matplotlib.pyplot as plt

import torchtext
from torchtext import data
import spacy

from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

if torch.cuda.is_available():
  torch.set_default_tensor_type(torch.cuda.FloatTensor)

from google.colab import drive

drive.mount('/content/gdrive')

import zipfile

!unzip "/content/gdrive/My Drive/data/onehot.zip"

# RNN Model (BINARY):
class RNN(nn.Module):
    def __init__(self, embedding_dim, vocab, hidden_dim):
        super(RNN, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)
        self.h_dim = hidden_dim
        self.GRU = nn.GRU(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)


    def forward(self, x, lengths=None):
        embedded = self.embedding(x)
        _, x = self.GRU(embedded)
        x = self.fc2(x)
        return x

# RNN Model (GENRE BINARY):

class RNN_G(nn.Module):
    def __init__(self, embedding_dim, vocab, hidden_dim):
        super(RNN, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)
        self.h_dim = hidden_dim
        self.GRU = nn.GRU(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 152)
        self.fc3 = nn.Linear(152, 1)


    def forward(self, x, genre, lengths=None):
        embedded = self.embedding(x)
        _, x = self.GRU(embedded)
        x = self.fc2(x)
        x = x + genre
        x = self.fc3(x)
        return x

# RNN Model (NEG NEUTRAL POSITIVE):

class RNN_NNN(nn.Module):
    def __init__(self, embedding_dim, vocab, hidden_dim):
        super(RNN, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)
        self.h_dim = hidden_dim
        self.GRU = nn.GRU(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 3)


    def forward(self, x, lengths=None):
        embedded = self.embedding(x)
        _, x = self.GRU(embedded)
        x = self.fc2(x)
        return x

# RNN Model (GENRE NEG NEUTRAL POSITIVE):

class RNN_GNNN(nn.Module):
    def __init__(self, embedding_dim, vocab, hidden_dim):
        super(RNN, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)
        self.h_dim = hidden_dim
        self.GRU = nn.GRU(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 152)
        self.fc3 = nn.Linear(152, 3)


    def forward(self, x, genre, lengths=None):
        embedded = self.embedding(x)
        _, x = self.GRU(embedded)
        x = self.fc2(x)
        x = x + genre
        x = self.fc3(x)
        return x

# RNN Model (1-5):

class RNN_15(nn.Module):
    def __init__(self, embedding_dim, vocab, hidden_dim):
        super(RNN, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)
        self.h_dim = hidden_dim
        self.GRU = nn.GRU(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 5)


    def forward(self, x, lengths=None):
        embedded = self.embedding(x)
        _, x = self.GRU(embedded)
        x = self.fc2(x)
        return x

# Validation and Testing Set Evaluation:
def evaluate(model, val_loader):
    v_accuracy = 0
    vloss = 0
    batch_t = 0
    for index, batch in enumerate(val_loader):
        data, length = batch.text
        truth = batch.label.float()
        #if torch.cuda.is_available():
            #data, truth = data.to(device), truth.to(device)
        y_valid = model(data, length).float()
        v_loss = bce_loss(y_valid.squeeze(), truth)
        vloss += v_loss.item()
        pred = (y_valid > 0.5).squeeze().float() == truth
        v_accuracy += int(pred.sum())
        batch_t += 1

    return v_accuracy / len(val_loader.dataset), vloss / batch_t

# Parameters:

batch_size = 128
lr = 0.01
epochs = 8
model = "rnn"

emb_dim = 100
rnn_hidden_dim = 100

n_filters = 10
filter_size = [2,4]

# Loading training data, tokenizing, and producing word vectors:
TEXT = data.Field(sequential=True,lower=True, tokenize='spacy', include_lengths=True)
LABELS = data.Field(sequential=False, use_vocab=False)

train_data, val_data, test_data = data.TabularDataset.splits(
            path='/content/', train='train.csv',
            validation='validation.csv', test='test.csv', format='csv',
            skip_header=True, fields=[('text', TEXT), ('label', LABELS)])

train_iter, val_iter, test_iter = data.BucketIterator.splits(
    (train_data, val_data, test_data), batch_sizes=(batch_size, batch_size, batch_size),
    sort_key=lambda x: len(x.text), device=None, sort_within_batch=True, repeat=False)

TEXT.build_vocab(train_data, val_data, test_data)
TEXT.vocab.load_vectors(torchtext.vocab.GloVe(name='6B', dim=100))
vocab = TEXT.vocab

print("Shape of Vocab:", TEXT.vocab.vectors.shape)

model = RNN(emb_dim, vocab, rnn_hidden_dim)

# Optimization method and loss function:
optimizer = optim.Adam(model.parameters(), lr=lr)
bce_loss = nn.BCEWithLogitsLoss()



#@title
# Validation and Testing Set Evaluation:
def evaluate(model, val_loader):
    v_accuracy = 0
    vloss = 0
    batch_t = 0
    for index, batch in enumerate(val_loader):
        data, length = batch.text
        truth = batch.label.float()
        #if torch.cuda.is_available():
            #data, truth = data.to(device), truth.to(device)
        y_valid = model(data, length).float()
        v_loss = bce_loss(y_valid.squeeze(), truth)
        vloss += v_loss.item()
        pred = (y_valid > 0.5).squeeze().float() == truth
        v_accuracy += int(pred.sum())
        batch_t += 1

    return v_accuracy / len(val_loader.dataset), vloss / batch_t

# Training loop used for all models:
training_accuracy = []
training_loss = []
valid_acc = []
valid_loss = []

model = model.cuda()



for epoch in range(epochs):
    epoch_loss = 0
    epoch_accuracy = 0
    batch_tracker = 0
    for index, batch in enumerate(train_iter):
        batch_input, batch_input_length = batch.text
        labels = batch.label.float()
        #labels = batch.negative, batch.neutral, batch.positive
        optimizer.zero_grad()

        y = model(batch_input, batch_input_length).float()
        loss = bce_loss(input=y.squeeze(), target=labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        pred = (y > 0.5).squeeze().float() == labels
        epoch_accuracy += int(pred.sum()) / len(labels)
        batch_tracker += 1
    training_loss += [epoch_loss / batch_tracker]
    training_accuracy += [epoch_accuracy / batch_tracker]
    print("Continue")
    
    # Evaluating the validation and testing data, as well as giving the losses and accuracies for all 3:
    if (epoch + 1) % 1 == 0:
        print("Epoch: ", f'{epoch + 1}', ", Training Loss: ", f'{epoch_loss / batch_tracker:.4f}',
              ", Training Accuracy: ", f'{epoch_accuracy / batch_tracker:.4f}')
        valid_accuracy, v_loss = evaluate(model, val_iter)
        valid_acc += [valid_accuracy]
        valid_loss += [v_loss]
        print("Epoch: ", f'{epoch + 1}', ", Validation Loss: ", f'{v_loss:.4f}',
              ", Validation Accuracy: ", f'{valid_accuracy:.4f}')

# Plotting the accuracies and loss of training, validation, and test datasets:
epoch_idx = np.arange(0, len(training_accuracy))

# Training-Validation Accuracy
plt.plot(epoch_idx, training_accuracy)
plt.plot(epoch_idx, valid_acc)
plt.xlabel('Number of Epochs')
plt.ylabel('Accuracy')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.title("RNN Training and Validation Accuracy")
plt.show()

# Training-Validation Loss
plt.plot(epoch_idx, training_loss)
plt.plot(epoch_idx, valid_loss)
plt.xlabel('Number of Epochs')
plt.ylabel('Loss')
plt.legend(['Training Loss', 'Validation Loss'])
plt.title("RNN Training and Validation Loss")
plt.show()

torch.save(model, "/content/model.pt")



prediction = []
t = []
with torch.no_grad(): 
    for index, batch in enumerate(val_iter):
        data, length = batch.text
        truth = batch.label.float()
        y_test = model(data, length).float()
        pred = (y_test > 0.5).squeeze().float() == truth
        prediction.append(pred)

confusion_matrix = np.zeros([2,2], int)
for batch in val_iter:
    data, length = batch.text
    truth = batch.label.float()
    c = 0
    for i, j in enumerate(truth):
      try:
        confusion_matrix[int(j.item()),int(prediction[c][i].item())] += 1 
        c += 1
      except:
        pass

fig, matrix = plt.subplots(1,1,figsize=(10,10))
classes = ('0', '1')
matrix.matshow(confusion_matrix, aspect='equal', cmap=plt.get_cmap('Greens'))
plt.xlabel('Ground Truth')
plt.ylabel('Predictions')
plt.xticks(range(2),classes)
plt.yticks(range(2),classes)
plt.title('Confusion Matrix of RNN for Binary Classification')
plt.show()



